{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original batch: tensor([[-1.0416, -1.9964, -0.6409,  0.7900, -1.8588]])\n",
      "New batch: tensor([[-1.0416, -1.9964, -0.6409,  0.7900],\n",
      "        [-1.9964, -0.6409,  0.7900, -1.8588]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# define batch and max_episode_len\n",
    "batch = [torch.randn(length) for length in [5]]\n",
    "max_episode_len = 4\n",
    "\n",
    "# convert batch to padded tensor\n",
    "padded_batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "# unfold along the time dimension to create fixed-length sequences\n",
    "# the size of the window is set to max_episode_len\n",
    "# the step size is set to 1 to include all possible sequences\n",
    "unfolded_batch = padded_batch.unfold(1, max_episode_len, 1)\n",
    "\n",
    "# reshape the unfolded tensor to create a new batch of fixed-length sequences\n",
    "# the size of the new batch is batch_size * num_sequences * sequence_length\n",
    "new_batch = unfolded_batch.reshape(-1, max_episode_len)\n",
    "\n",
    "# print the original batch and the new batch\n",
    "print(\"Original batch:\", padded_batch)\n",
    "print(\"New batch:\", new_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-1.0416, -1.9964, -0.6409,  0.7900, -1.8588]]), torch.Size([1, 5]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_batch, padded_batch.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = torch.randn(10,3,2)\n",
    "dims = s.size()\n",
    "s1 = s.unfold(dimension=0, size=4, step=1).permute(0, -1, *(torch.arange(1, len(dims))))\n",
    "s,s1,s1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 3, 2])"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "a = np.random.randn(10,3,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 3, 2)"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([a,a]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [217]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m list_of_tensors \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m), torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m3\u001B[39m), torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Pad all tensors to the same sequence length\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m padded_tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlist_of_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Stack the padded tensors vertically\u001B[39;00m\n\u001B[1;32m     11\u001B[0m stacked_tensors \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([tensor\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m padded_tensors], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/nn/utils/rnn.py:399\u001B[0m, in \u001B[0;36mpad_sequence\u001B[0;34m(sequences, batch_first, padding_value)\u001B[0m\n\u001B[1;32m    395\u001B[0m         sequences \u001B[38;5;241m=\u001B[39m sequences\u001B[38;5;241m.\u001B[39munbind(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001B[39;00m\n\u001B[0;32m--> 399\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Suppose you have a list of tensors with different shapes\n",
    "list_of_tensors = [torch.randn(1, 2), torch.randn(5, 3), torch.randn(8, 1)]\n",
    "\n",
    "# Pad all tensors to the same sequence length\n",
    "padded_tensors = pad_sequence(list_of_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "# Stack the padded tensors vertically\n",
    "stacked_tensors = torch.cat([tensor.unsqueeze(0) for tensor in padded_tensors], dim=0)\n",
    "\n",
    "# Print the shape of the stacked tensors\n",
    "print(stacked_tensors.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "def pad_tensor(x, target_len):\n",
    "    dim = x.dim()\n",
    "\n",
    "    pad = [0] * dim * 2\n",
    "\n",
    "    x = F.pad(x, (*pad, 0, target_len - x.size(1)), 'constant', 0)\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "# pad_tensor(torch.randn(200,5,4), target_len=10)\n",
    "x = torch.randn(200,5,4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = x.dim()\n",
    "dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "i = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "pad = [0] * (dim - i - 1) * 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0]"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([200, 20, 4])"
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(x, (*pad, 0, 20 - x.size(1)), 'constant', 0).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "\n",
    "def pad_tensor(x, dim, target_len):\n",
    "    n_dim = x.dim()\n",
    "\n",
    "    pad = [0] * (n_dim - dim - 1) * 2\n",
    "    padding_length = target_len - x.size(dim)\n",
    "\n",
    "    x = F.pad(x, (*pad, 0, padding_length), 'constant', 0)\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 5, 6])"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tensor(torch.randn(1,5,6), dim=0, target_len=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}